
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Polish Less, Verify More</title>
    <style>
        body {
            font-family: Georgia, serif;
            max-width: 750px;
            margin: 40px auto;
            line-height: 1.65;
            color: #333;
            padding: 0 20px;
        }
        h1 {
            font-size: 28px;
            color: #2E86AB;
        }
        img {
            display: block;
            margin: 30px auto;
            max-width: 100%;
            height: auto;
        }
        figcaption {
            text-align: center;
            font-style: italic;
            margin-top: 10px;
            color: #666;
        }
        blockquote {
            font-style: italic;
            margin: 20px 0;
            padding-left: 20px;
            border-left: 4px solid #ccc;
            color: #555;
        }
    </style>
</head>
<body>
    <h1>Polish Less, Verify More: Recalibrating Assessment in the Age of AI</h1>

    <p>Generative AI tools like ChatGPT are redefining the writing process for today’s students. With support in syntax, grammar, and organization, learners can now produce more coherent and polished drafts than ever before—<em>yes, even with that telltale em dash</em>. This fluency is a welcome development, but it shifts the pedagogical emphasis away from surface-level polish and toward more substantive intellectual rigor.</p>

    <p>Rather than focusing primarily on grammatical correctness or rhetorical flair, we now have the opportunity to foreground competencies such as <strong>information literacy</strong>, <strong>critical analysis</strong>, and <strong>source validation</strong>. In short, the measure of writing success must increasingly reflect a student’s ability to engage responsibly with AI outputs.</p>

    <p>This shift invites several pedagogical opportunities:</p>
    <ul>
        <li>Encouraging students to interrogate AI-generated content rather than accept it at face value</li>
        <li>Integrating fact-checking and citation practices as standard components of AI-assisted work</li>
        <li>Evaluating claims in relation to disciplinary frameworks and empirical evidence</li>
        <li>Cultivating metacognitive awareness of what AI can—and cannot—reliably generate</li>
    </ul>

    <p>To reflect this evolution in student writing and scholarly engagement, our assessment criteria must shift accordingly:</p>

    <figure>
        <img src="accuracy_pie_chart_rotated.svg" alt="Pie chart showing 90% for accuracy and 10% for fluency, rotated for better label placement">
        <figcaption>In the AI era, fluency is table stakes—credibility is the true benchmark of mastery.</figcaption>
    </figure>

    <p>While we continue to value clarity and readability, the weight of evaluation now rests more squarely on the learner’s ability to verify content, synthesize knowledge, and produce defensible arguments grounded in evidence. This aligns well with graduate-level expectations and faculty-level standards of academic integrity.</p>

    <blockquote>
        “AI can polish your prose, but it can’t verify your facts. That’s your responsibility.”
    </blockquote>

    <p>This reframing is not about penalizing AI use—it’s about leveraging it effectively. For educators and researchers alike, the challenge now is to design writing tasks and evaluation frameworks that hold space for both innovation and intellectual accountability.</p>
</body>
</html>
